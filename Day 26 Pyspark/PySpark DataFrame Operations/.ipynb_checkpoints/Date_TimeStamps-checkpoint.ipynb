{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2209c9b-d394-471d-ba9a-21f770978480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/09 17:48:48 WARN Utils: Your hostname, MaheshPC resolves to a loopback address: 127.0.1.1; using 192.168.1.7 instead (on interface enp5s0)\n",
      "24/01/09 17:48:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/09 17:48:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/09 17:48:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/01/09 17:48:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/01/09 17:48:49 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/01/09 17:48:49 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/01/09 17:48:49 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/01/09 17:48:49 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is Connected\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ops4').getOrCreate()\n",
    "print('Spark is Connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdd76f6-efae-4e42-8541-34c8a2461172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Datasets/appl_stock.csv',inferSchema=True,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba354cd5-bd3e-47f0-9a59-87c415b21d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Date|              Open|\n",
      "+----------+------------------+\n",
      "|2010-01-04|        213.429998|\n",
      "|2010-01-05|        214.599998|\n",
      "|2010-01-06|        214.379993|\n",
      "|2010-01-07|            211.75|\n",
      "|2010-01-08|        210.299994|\n",
      "|2010-01-11|212.79999700000002|\n",
      "|2010-01-12|209.18999499999998|\n",
      "|2010-01-13|        207.870005|\n",
      "|2010-01-14|210.11000299999998|\n",
      "|2010-01-15|210.92999500000002|\n",
      "|2010-01-19|        208.330002|\n",
      "|2010-01-20|        214.910006|\n",
      "|2010-01-21|        212.079994|\n",
      "|2010-01-22|206.78000600000001|\n",
      "|2010-01-25|202.51000200000001|\n",
      "|2010-01-26|205.95000100000001|\n",
      "|2010-01-27|        206.849995|\n",
      "|2010-01-28|        204.930004|\n",
      "|2010-01-29|        201.079996|\n",
      "|2010-02-01|192.36999699999998|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_d = df.select(['Date','Open'])\n",
    "df_d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2fbb47-54b8-46b5-ab0a-ef5b606c16cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+\n",
      "|      Date|              Open|DayOfMonth|\n",
      "+----------+------------------+----------+\n",
      "|2010-01-04|        213.429998|         4|\n",
      "|2010-01-05|        214.599998|         5|\n",
      "|2010-01-06|        214.379993|         6|\n",
      "|2010-01-07|            211.75|         7|\n",
      "|2010-01-08|        210.299994|         8|\n",
      "|2010-01-11|212.79999700000002|        11|\n",
      "|2010-01-12|209.18999499999998|        12|\n",
      "|2010-01-13|        207.870005|        13|\n",
      "|2010-01-14|210.11000299999998|        14|\n",
      "|2010-01-15|210.92999500000002|        15|\n",
      "|2010-01-19|        208.330002|        19|\n",
      "|2010-01-20|        214.910006|        20|\n",
      "|2010-01-21|        212.079994|        21|\n",
      "|2010-01-22|206.78000600000001|        22|\n",
      "|2010-01-25|202.51000200000001|        25|\n",
      "|2010-01-26|205.95000100000001|        26|\n",
      "|2010-01-27|        206.849995|        27|\n",
      "|2010-01-28|        204.930004|        28|\n",
      "|2010-01-29|        201.079996|        29|\n",
      "|2010-02-01|192.36999699999998|         1|\n",
      "+----------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth,year,dayofyear,month,weekofyear,hour,format_number\n",
    "\n",
    "#df_d.withColumn('year',year(df_d['Date'])).show()\n",
    "\n",
    "#df_d.withColumn('Month',dayofmonth(df_d['Date'])).show()\n",
    "\n",
    "#df_d.withColumn('WeekOfYear',weekofyear(df_d['Date'])).show()\n",
    "\n",
    "#df_d.withColumn('DayOfyear',dayofyear(df_d['Date'])).show()\n",
    "\n",
    "df_d.withColumn('DayOfMonth',dayofmonth(df_d['Date'])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b65adf-e996-49c1-9906-4481da16ae21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
