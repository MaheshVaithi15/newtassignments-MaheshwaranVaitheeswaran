{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42302451-f47b-48ec-9662-2a92be4544d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/12 10:43:44 WARN Utils: Your hostname, MaheshPC resolves to a loopback address: 127.0.1.1; using 192.168.1.7 instead (on interface enp5s0)\n",
      "24/01/12 10:43:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/12 10:43:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session is created\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('opt').getOrCreate()\n",
    "print('Spark session is created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eea2d46-838e-47e0-a879-d6384816ec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760b'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join stratagies\n",
    "\n",
    "# broadcast join\n",
    "\n",
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')\n",
    "\n",
    "# int(spark.conf.get('spark.sql.autoBroadcastJoinThreshold'))/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30be313-b64b-479a-9851-fd79df95fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+---------------+\n",
      "|ordid|            orddate|ordcustid|      ordstatus|\n",
      "+-----+-------------------+---------+---------------+\n",
      "|    1|2013-07-25 00:00:00|    11599|         CLOSED|\n",
      "|    2|2013-07-25 00:00:00|      256|PENDING_PAYMENT|\n",
      "|    3|2013-07-25 00:00:00|    12111|       COMPLETE|\n",
      "|    4|2013-07-25 00:00:00|     8827|         CLOSED|\n",
      "|    5|2013-07-25 00:00:00|    11318|       COMPLETE|\n",
      "|    6|2013-07-25 00:00:00|     7130|       COMPLETE|\n",
      "|    7|2013-07-25 00:00:00|     4530|       COMPLETE|\n",
      "|    8|2013-07-25 00:00:00|     2911|     PROCESSING|\n",
      "|    9|2013-07-25 00:00:00|     5657|PENDING_PAYMENT|\n",
      "|   10|2013-07-25 00:00:00|     5648|PENDING_PAYMENT|\n",
      "|   11|2013-07-25 00:00:00|      918| PAYMENT_REVIEW|\n",
      "|   12|2013-07-25 00:00:00|     1837|         CLOSED|\n",
      "|   13|2013-07-25 00:00:00|     9149|PENDING_PAYMENT|\n",
      "|   14|2013-07-25 00:00:00|     9842|     PROCESSING|\n",
      "|   15|2013-07-25 00:00:00|     2568|       COMPLETE|\n",
      "|   16|2013-07-25 00:00:00|     7276|PENDING_PAYMENT|\n",
      "|   17|2013-07-25 00:00:00|     2667|       COMPLETE|\n",
      "|   18|2013-07-25 00:00:00|     1205|         CLOSED|\n",
      "|   19|2013-07-25 00:00:00|     9488|PENDING_PAYMENT|\n",
      "|   20|2013-07-25 00:00:00|     9198|     PROCESSING|\n",
      "+-----+-------------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing datasets\n",
    "\n",
    "ord = spark.read.load('Datasets/Orders/part-00000',sep=',',format='csv',schema=('ordid int,orddate timestamp,ordcustid int,ordstatus string'))\n",
    "\n",
    "ord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025bf8f8-4fa0-46b9-b085-ddd81c6c9f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+--------+\n",
      "|orditemid|itemordid|prodid|quantity|subtotal|\n",
      "+---------+---------+------+--------+--------+\n",
      "|        1|        1|   957|       1|  299.98|\n",
      "|        2|        2|  1073|       1|  199.99|\n",
      "|        3|        2|   502|       5|   250.0|\n",
      "|        4|        2|   403|       1|  129.99|\n",
      "|        5|        4|   897|       2|   49.98|\n",
      "|        6|        4|   365|       5|  299.95|\n",
      "|        7|        4|   502|       3|   150.0|\n",
      "|        8|        4|  1014|       4|  199.92|\n",
      "|        9|        5|   957|       1|  299.98|\n",
      "|       10|        5|   365|       5|  299.95|\n",
      "|       11|        5|  1014|       2|   99.96|\n",
      "|       12|        5|   957|       1|  299.98|\n",
      "|       13|        5|   403|       1|  129.99|\n",
      "|       14|        7|  1073|       1|  199.99|\n",
      "|       15|        7|   957|       1|  299.98|\n",
      "|       16|        7|   926|       5|   79.95|\n",
      "|       17|        8|   365|       3|  179.97|\n",
      "|       18|        8|   365|       5|  299.95|\n",
      "|       19|        8|  1014|       4|  199.92|\n",
      "|       20|        8|   502|       1|    50.0|\n",
      "+---------+---------+------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orditem = spark.read.load('Datasets/Order_items/part-00000',sep=',',format='csv',schema=('orditemid int,itemordid int,prodid int,quantity int,subtotal float'))\n",
    "\n",
    "orditem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8963c0fc-9a39-4aa0-971d-b82ec5f4d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+---------------+---------+---------+------+--------+--------+\n",
      "|ordid|            orddate|ordcustid|      ordstatus|orditemid|itemordid|prodid|quantity|subtotal|\n",
      "+-----+-------------------+---------+---------------+---------+---------+------+--------+--------+\n",
      "|    1|2013-07-25 00:00:00|    11599|         CLOSED|        1|        1|   957|       1|  299.98|\n",
      "|    2|2013-07-25 00:00:00|      256|PENDING_PAYMENT|        2|        2|  1073|       1|  199.99|\n",
      "|    3|2013-07-25 00:00:00|    12111|       COMPLETE|        3|        2|   502|       5|   250.0|\n",
      "|    4|2013-07-25 00:00:00|     8827|         CLOSED|        4|        2|   403|       1|  129.99|\n",
      "|    5|2013-07-25 00:00:00|    11318|       COMPLETE|        5|        4|   897|       2|   49.98|\n",
      "|    6|2013-07-25 00:00:00|     7130|       COMPLETE|        6|        4|   365|       5|  299.95|\n",
      "|    7|2013-07-25 00:00:00|     4530|       COMPLETE|        7|        4|   502|       3|   150.0|\n",
      "|    8|2013-07-25 00:00:00|     2911|     PROCESSING|        8|        4|  1014|       4|  199.92|\n",
      "|    9|2013-07-25 00:00:00|     5657|PENDING_PAYMENT|        9|        5|   957|       1|  299.98|\n",
      "|   10|2013-07-25 00:00:00|     5648|PENDING_PAYMENT|       10|        5|   365|       5|  299.95|\n",
      "|   11|2013-07-25 00:00:00|      918| PAYMENT_REVIEW|       11|        5|  1014|       2|   99.96|\n",
      "|   12|2013-07-25 00:00:00|     1837|         CLOSED|       12|        5|   957|       1|  299.98|\n",
      "|   13|2013-07-25 00:00:00|     9149|PENDING_PAYMENT|       13|        5|   403|       1|  129.99|\n",
      "|   14|2013-07-25 00:00:00|     9842|     PROCESSING|       14|        7|  1073|       1|  199.99|\n",
      "|   15|2013-07-25 00:00:00|     2568|       COMPLETE|       15|        7|   957|       1|  299.98|\n",
      "|   16|2013-07-25 00:00:00|     7276|PENDING_PAYMENT|       16|        7|   926|       5|   79.95|\n",
      "|   17|2013-07-25 00:00:00|     2667|       COMPLETE|       17|        8|   365|       3|  179.97|\n",
      "|   18|2013-07-25 00:00:00|     1205|         CLOSED|       18|        8|   365|       5|  299.95|\n",
      "|   19|2013-07-25 00:00:00|     9488|PENDING_PAYMENT|       19|        8|  1014|       4|  199.92|\n",
      "|   20|2013-07-25 00:00:00|     9198|     PROCESSING|       20|        8|   502|       1|    50.0|\n",
      "+-----+-------------------+---------+---------------+---------+---------+------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to perform broadcast join\n",
    "\n",
    "joined_ord = ord.join(orditem,ord.ordid == orditem.orditemid)\n",
    "\n",
    "joined_ord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecc710ff-1f53-4450-b8c5-bcfefe0c3a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [ordid#47], [orditemid#138], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=182]\n",
      "   :  +- Filter isnotnull(ordid#47)\n",
      "   :     +- FileScan csv [ordid#47,orddate#48,ordcustid#49,ordstatus#50] Batched: false, DataFilters: [isnotnull(ordid#47)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mahesh/Python Projects/PySpark Day 5/PySpark Opitimization ..., PartitionFilters: [], PushedFilters: [IsNotNull(ordid)], ReadSchema: struct<ordid:int,orddate:timestamp,ordcustid:int,ordstatus:string>\n",
      "   +- Filter isnotnull(orditemid#138)\n",
      "      +- FileScan csv [orditemid#138,itemordid#139,prodid#140,quantity#141,subtotal#142] Batched: false, DataFilters: [isnotnull(orditemid#138)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mahesh/Python Projects/PySpark Day 5/PySpark Opitimization ..., PartitionFilters: [], PushedFilters: [IsNotNull(orditemid)], ReadSchema: struct<orditemid:int,itemordid:int,prodid:int,quantity:int,subtotal:float>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_ord.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04832b7d-1eae-4482-8bcc-24983c754900",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "736e055f-ba17-49f2-a7ef-ee216ae36489",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ord2 =ord.join(orditem,ord.ordid == orditem.orditemid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2405035c-0d32-4997-bed6-b6f991ffb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [ordid#47], [orditemid#138], Inner\n",
      "   :- Sort [ordid#47 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(ordid#47, 200), ENSURE_REQUIREMENTS, [plan_id=206]\n",
      "   :     +- Filter isnotnull(ordid#47)\n",
      "   :        +- FileScan csv [ordid#47,orddate#48,ordcustid#49,ordstatus#50] Batched: false, DataFilters: [isnotnull(ordid#47)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mahesh/Python Projects/PySpark Day 5/PySpark Opitimization ..., PartitionFilters: [], PushedFilters: [IsNotNull(ordid)], ReadSchema: struct<ordid:int,orddate:timestamp,ordcustid:int,ordstatus:string>\n",
      "   +- Sort [orditemid#138 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(orditemid#138, 200), ENSURE_REQUIREMENTS, [plan_id=207]\n",
      "         +- Filter isnotnull(orditemid#138)\n",
      "            +- FileScan csv [orditemid#138,itemordid#139,prodid#140,quantity#141,subtotal#142] Batched: false, DataFilters: [isnotnull(orditemid#138)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mahesh/Python Projects/PySpark Day 5/PySpark Opitimization ..., PartitionFilters: [], PushedFilters: [IsNotNull(orditemid)], ReadSchema: struct<orditemid:int,itemordid:int,prodid:int,quantity:int,subtotal:float>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_ord2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f5ea22-7af8-4ae3-987e-f299053e961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "largedf = spark.range(1,100000)\n",
    "\n",
    "d_l = [('a',1),('b',2)]\n",
    "\n",
    "sch_l = ['col','id']\n",
    "\n",
    "df_l = spark.createDataFrame(data=d_l,schema=sch_l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbfbcd83-7887-4e7c-bb06-219ae4e21f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|col| id|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_l.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a82ca6dd-8acc-4c9d-8d8a-69e502f29bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|col|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined2 = largedf.join(df_l,'id')\n",
    "\n",
    "joined2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393ed9e0-c87a-4170-b078-433dc6de8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#257L, col#259]\n",
      "   +- SortMergeJoin [id#257L], [id#260L], Inner\n",
      "      :- Sort [id#257L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#257L, 200), ENSURE_REQUIREMENTS, [plan_id=354]\n",
      "      :     +- Range (1, 100000, step=1, splits=8)\n",
      "      +- Sort [id#260L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#260L, 200), ENSURE_REQUIREMENTS, [plan_id=353]\n",
      "            +- Filter isnotnull(id#260L)\n",
      "               +- Scan ExistingRDD[col#259,id#260L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fc35305-90a6-4d46-b5a4-3c21bce4ce73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|col|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another way to perform join is using hint keyword\n",
    "\n",
    "joined3 = largedf.hint('BROADCAST').join(df_l,'id')\n",
    "joined3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58c990f8-d3b0-45d4-b8e2-de15c797eaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#257L, col#259]\n",
      "   +- BroadcastHashJoin [id#257L], [id#260L], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=436]\n",
      "      :  +- Range (1, 100000, step=1, splits=8)\n",
      "      +- Filter isnotnull(id#260L)\n",
      "         +- Scan ExistingRDD[col#259,id#260L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a763596b-6027-4c38-b3cf-88fedf977e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.join.preferSortMergeJoin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "859e50e0-fa51-4e77-a422-b7fd60e1beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.join.preferSortMergeJoin',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a92f89-985f-49aa-b8c7-e49446c7374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#296L]\n",
      "   +- SortMergeJoin [id#296L], [id#298L], Inner\n",
      "      :- Sort [id#296L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#296L, 200), ENSURE_REQUIREMENTS, [plan_id=456]\n",
      "      :     +- Range (1, 10000000, step=1, splits=8)\n",
      "      +- Sort [id#298L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#298L, 200), ENSURE_REQUIREMENTS, [plan_id=457]\n",
      "            +- Range (1, 100000, step=1, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1 = spark.range(1,10000000)\n",
    "l2 = spark.range(1,100000)\n",
    "\n",
    "joined4 = l1.join(l2,'id')\n",
    "\n",
    "joined4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f711716-7cfd-4e9f-b16f-805e75f6094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.join.preferSortMergeJoin',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf03df45-6a2a-4e5c-af84-035e1506579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#296L]\n",
      "   +- SortMergeJoin [id#296L], [id#298L], Inner\n",
      "      :- Sort [id#296L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#296L, 200), ENSURE_REQUIREMENTS, [plan_id=479]\n",
      "      :     +- Range (1, 10000000, step=1, splits=8)\n",
      "      +- Sort [id#298L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#298L, 200), ENSURE_REQUIREMENTS, [plan_id=480]\n",
      "            +- Range (1, 100000, step=1, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shuffle sort join\n",
    "\n",
    "joined44 = l1.join(l2,'id')\n",
    "\n",
    "joined44.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6e694-7de4-4baf-bd6e-a703fa7bc5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
