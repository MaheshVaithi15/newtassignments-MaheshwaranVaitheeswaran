{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f010ef0-7de1-444f-b54e-57a346c47e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/10 12:37:49 WARN Utils: Your hostname, MaheshPC resolves to a loopback address: 127.0.1.1; using 192.168.1.7 instead (on interface enp5s0)\n",
      "24/01/10 12:37:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/10 12:37:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/10 12:37:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/01/10 12:37:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session is Created...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Features').getOrCreate()\n",
    "print('Spark Session is Created...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6c21af-cc69-420b-a5ec-008b293194a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RuntimeConfig in module pyspark.sql.conf object:\n",
      "\n",
      "class RuntimeConfig(builtins.object)\n",
      " |  RuntimeConfig(jconf: py4j.java_gateway.JavaObject) -> None\n",
      " |  \n",
      " |  User-facing configuration API, accessible through `SparkSession.conf`.\n",
      " |  \n",
      " |  Options set here are automatically propagated to the Hadoop configuration during I/O.\n",
      " |  \n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, jconf: py4j.java_gateway.JavaObject) -> None\n",
      " |      Create a new RuntimeConfig that wraps the underlying JVM object.\n",
      " |  \n",
      " |  get(self, key: str, default: Union[str, NoneType, pyspark._globals._NoValueType] = <no value>) -> Optional[str]\n",
      " |      Returns the value of Spark runtime configuration property for the given key,\n",
      " |      assuming it is set.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  isModifiable(self, key: str) -> bool\n",
      " |      Indicates whether the configuration property with the given key\n",
      " |      is modifiable in the current session.\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  set(self, key: str, value: Union[str, int, bool]) -> None\n",
      " |      Sets the given Spark runtime configuration property.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unset(self, key: str) -> None\n",
      " |      Resets the configuration property for the given key.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ae110e3-b6f7-4f64-8d96-5f7fe589c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark session conf()\n",
    "\n",
    "partitions = spark.conf.get('spark.sql.shuffle.partitions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd4e6e9-3f4c-4b85-bb31-bf77e8dcb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.yarn.appMasterEnv.HDFS_PATH','Datasets/Orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d1498e-ff2c-4854-a909-2d47ebdba8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = spark.conf.get('spark.yarn.appMasterEnv.HDFS_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2185ff56-7adb-42eb-9bdd-92eed2ef10fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Datasets/Orders\n"
     ]
    }
   ],
   "source": [
    "print(partitions)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ab723b-fb5a-4dd8-8662-5454d4ac575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on UDFRegistration in module pyspark.sql.udf object:\n",
      "\n",
      "class UDFRegistration(builtins.object)\n",
      " |  UDFRegistration(sparkSession: 'SparkSession')\n",
      " |  \n",
      " |  Wrapper for user-defined function registration. This instance can be accessed by\n",
      " |  :attr:`spark.udf` or :attr:`sqlContext.udf`.\n",
      " |  \n",
      " |  .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkSession: 'SparkSession')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  register(self, name: str, f: Union[Callable[..., Any], ForwardRef('UserDefinedFunctionLike')], returnType: Optional[ForwardRef('DataTypeOrString')] = None) -> 'UserDefinedFunctionLike'\n",
      " |      Register a Python function (including lambda function) or a user-defined function\n",
      " |      as a SQL function.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str,\n",
      " |          name of the user-defined function in SQL statements.\n",
      " |      f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n",
      " |          a Python function, or a user-defined function. The user-defined function can\n",
      " |          be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n",
      " |          :meth:`pyspark.sql.functions.pandas_udf`.\n",
      " |      returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      " |          the return type of the registered user-defined function. The value can\n",
      " |          be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |          `returnType` can be optionally specified when `f` is a Python function but not\n",
      " |          when `f` is a user-defined function. Please see the examples below.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      function\n",
      " |          a user-defined function\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To register a nondeterministic Python function, users need to first build\n",
      " |      a nondeterministic user-defined function for the Python function and then register it\n",
      " |      as a SQL function.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      1. When `f` is a Python function:\n",
      " |      \n",
      " |          `returnType` defaults to string type and can be optionally specified. The produced\n",
      " |          object must match the specified type. In this case, this API works as if\n",
      " |          `register(name, f, returnType=StringType())`.\n",
      " |      \n",
      " |          >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n",
      " |          >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n",
      " |          [Row(stringLengthString(test)='4')]\n",
      " |      \n",
      " |          >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n",
      " |          [Row(stringLengthString(text)='3')]\n",
      " |      \n",
      " |          >>> from pyspark.sql.types import IntegerType\n",
      " |          >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      " |          >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      " |          [Row(stringLengthInt(test)=4)]\n",
      " |      \n",
      " |          >>> from pyspark.sql.types import IntegerType\n",
      " |          >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      " |          >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      " |          [Row(stringLengthInt(test)=4)]\n",
      " |      \n",
      " |      2. When `f` is a user-defined function (from Spark 2.3.0):\n",
      " |      \n",
      " |          Spark uses the return type of the given user-defined function as the return type of\n",
      " |          the registered user-defined function. `returnType` should not be specified.\n",
      " |          In this case, this API works as if `register(name, f)`.\n",
      " |      \n",
      " |          >>> from pyspark.sql.types import IntegerType\n",
      " |          >>> from pyspark.sql.functions import udf\n",
      " |          >>> slen = udf(lambda s: len(s), IntegerType())\n",
      " |          >>> _ = spark.udf.register(\"slen\", slen)\n",
      " |          >>> spark.sql(\"SELECT slen('test')\").collect()\n",
      " |          [Row(slen(test)=4)]\n",
      " |      \n",
      " |          >>> import random\n",
      " |          >>> from pyspark.sql.functions import udf\n",
      " |          >>> from pyspark.sql.types import IntegerType\n",
      " |          >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n",
      " |          >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n",
      " |          >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n",
      " |          [Row(random_udf()=82)]\n",
      " |      \n",
      " |          >>> import pandas as pd  # doctest: +SKIP\n",
      " |          >>> from pyspark.sql.functions import pandas_udf\n",
      " |          >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      " |          ... def add_one(s: pd.Series) -> pd.Series:\n",
      " |          ...     return s + 1\n",
      " |          ...\n",
      " |          >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n",
      " |          >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n",
      " |          [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n",
      " |      \n",
      " |          >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      " |          ... def sum_udf(v: pd.Series) -> int:\n",
      " |          ...     return v.sum()\n",
      " |          ...\n",
      " |          >>> _ = spark.udf.register(\"sum_udf\", sum_udf)  # doctest: +SKIP\n",
      " |          >>> q = \"SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2\"\n",
      " |          >>> spark.sql(q).collect()  # doctest: +SKIP\n",
      " |          [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n",
      " |  \n",
      " |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[ForwardRef('DataTypeOrString')] = None) -> None\n",
      " |      Register a Java user-defined function as a SQL function.\n",
      " |      \n",
      " |      In addition to a name and the function itself, the return type can be optionally specified.\n",
      " |      When the return type is not specified we would infer it via reflection.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          name of the user-defined function\n",
      " |      javaClassName : str\n",
      " |          fully qualified name of java class\n",
      " |      returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      " |          the return type of the registered Java function. The value can be either\n",
      " |          a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.types import IntegerType\n",
      " |      >>> spark.udf.registerJavaFunction(\n",
      " |      ...     \"javaStringLength\", \"test.org.apache.spark.sql.JavaStringLength\", IntegerType())\n",
      " |      ... # doctest: +SKIP\n",
      " |      >>> spark.sql(\"SELECT javaStringLength('test')\").collect()  # doctest: +SKIP\n",
      " |      [Row(javaStringLength(test)=4)]\n",
      " |      \n",
      " |      >>> spark.udf.registerJavaFunction(\n",
      " |      ...     \"javaStringLength2\", \"test.org.apache.spark.sql.JavaStringLength\")\n",
      " |      ... # doctest: +SKIP\n",
      " |      >>> spark.sql(\"SELECT javaStringLength2('test')\").collect()  # doctest: +SKIP\n",
      " |      [Row(javaStringLength2(test)=4)]\n",
      " |      \n",
      " |      >>> spark.udf.registerJavaFunction(\n",
      " |      ...     \"javaStringLength3\", \"test.org.apache.spark.sql.JavaStringLength\", \"integer\")\n",
      " |      ... # doctest: +SKIP\n",
      " |      >>> spark.sql(\"SELECT javaStringLength3('test')\").collect()  # doctest: +SKIP\n",
      " |      [Row(javaStringLength3(test)=4)]\n",
      " |  \n",
      " |  registerJavaUDAF(self, name: str, javaClassName: str) -> None\n",
      " |      Register a Java user-defined aggregate function as a SQL function.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      name : str\n",
      " |          name of the user-defined aggregate function\n",
      " |      javaClassName : str\n",
      " |          fully qualified name of java class\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.udf.registerJavaUDAF(\"javaUDAF\", \"test.org.apache.spark.sql.MyDoubleAvg\")\n",
      " |      ... # doctest: +SKIP\n",
      " |      >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"a\")],[\"id\", \"name\"])\n",
      " |      >>> df.createOrReplaceTempView(\"df\")\n",
      " |      >>> q = \"SELECT name, javaUDAF(id) as avg from df group by name order by name desc\"\n",
      " |      >>> spark.sql(q).collect()  # doctest: +SKIP\n",
      " |      [Row(name='b', avg=102.0), Row(name='a', avg=102.0)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# udf in spark session\n",
    "\n",
    "help(spark.udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b63a14cb-533e-4cdc-aa10-a9bac9c5ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function is Created\n"
     ]
    }
   ],
   "source": [
    "# spark udf is user defined functions \n",
    "# create udf and register the udf in dataframe and saprk sql\n",
    "\n",
    "import string\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def cap(str):\n",
    "    finalStr = \"\"\n",
    "    ar = str.split(' ')\n",
    "    for word in ar:\n",
    "        finalStr = finalStr + word[:1].upper() + word[1:len(word)] + ' '\n",
    "\n",
    "    return string.strip(finalStr)\n",
    "\n",
    "print('Function is Created')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b5e87d6-b1c2-4b1a-9c42-3891a99a2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Pradeep| 25|\n",
      "| Pravin| 24|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to create dataframe from the function\n",
    "lst1 = [('Pradeep','25'),('Pravin','24')]\n",
    "df_emp = spark.createDataFrame(data=lst1,schema=['name','age'])\n",
    "\n",
    "df_emp.show()\n",
    "\n",
    "#df_emp.select(df_emp.name,cap(df_emp.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36359370-c0b7-4022-b7fd-37776c9cc843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/10 13:10:58 WARN SimpleFunctionRegistry: The function cap replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# to create in sql\n",
    "\n",
    "spark.udf.register('cap',cap)\n",
    "\n",
    "df_emp.createOrReplaceTempView('emp')\n",
    "\n",
    "#spark.sql(\"\"\"select name,cap(name) from default.emp\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b0ade78-5678-4cc3-a98b-aaaf528e01b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-----+---------------+\n",
      "|  1|2013-07-25 00:00:00.0|11599|         CLOSED|\n",
      "+---+---------------------+-----+---------------+\n",
      "|  2|  2013-07-25 00:00:00|  256|PENDING_PAYMENT|\n",
      "|  3|  2013-07-25 00:00:00|12111|       COMPLETE|\n",
      "|  4|  2013-07-25 00:00:00| 8827|         CLOSED|\n",
      "|  5|  2013-07-25 00:00:00|11318|       COMPLETE|\n",
      "|  6|  2013-07-25 00:00:00| 7130|       COMPLETE|\n",
      "|  7|  2013-07-25 00:00:00| 4530|       COMPLETE|\n",
      "|  8|  2013-07-25 00:00:00| 2911|     PROCESSING|\n",
      "|  9|  2013-07-25 00:00:00| 5657|PENDING_PAYMENT|\n",
      "| 10|  2013-07-25 00:00:00| 5648|PENDING_PAYMENT|\n",
      "| 11|  2013-07-25 00:00:00|  918| PAYMENT_REVIEW|\n",
      "| 12|  2013-07-25 00:00:00| 1837|         CLOSED|\n",
      "| 13|  2013-07-25 00:00:00| 9149|PENDING_PAYMENT|\n",
      "| 14|  2013-07-25 00:00:00| 9842|     PROCESSING|\n",
      "| 15|  2013-07-25 00:00:00| 2568|       COMPLETE|\n",
      "| 16|  2013-07-25 00:00:00| 7276|PENDING_PAYMENT|\n",
      "| 17|  2013-07-25 00:00:00| 2667|       COMPLETE|\n",
      "| 18|  2013-07-25 00:00:00| 1205|         CLOSED|\n",
      "| 19|  2013-07-25 00:00:00| 9488|PENDING_PAYMENT|\n",
      "| 20|  2013-07-25 00:00:00| 9198|     PROCESSING|\n",
      "| 21|  2013-07-25 00:00:00| 2711|        PENDING|\n",
      "+---+---------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading csv json text etc... from spark\n",
    "\n",
    "df_csv = spark.read.csv('Datasets/Orders/part-00000',inferSchema=True,header=True)\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26fa2c1e-c5db-4999-b569-4d7cf740ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2,Fitness\\n3,Foot...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading text files\n",
    "\n",
    "df_txt = spark.read.load(path='Datasets/Departments/part-00000',format='text',wholeText=True)\n",
    "\n",
    "df_txt.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f29efe29-7419-4eae-8cf3-b53167c10b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file in orc and parquet format\n",
    "# df_orc = spark.read.load(path='Datasets/Departments/part-00000',format='orc')\n",
    "# df_pq = spark.read.load(path='Datasets/Departments/part-00000',format='parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d34e04c-762d-4b7e-88f2-9671ad23d170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|Age|  Name|\n",
      "+---+------+\n",
      "| 25|Mahesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read json \n",
    "\n",
    "df_json = spark.read.load(path='Datasets/demo.json',format='json')\n",
    "\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c22185-f1ad-4aa3-bb7f-77cb846e3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read avro,hive,\n",
    "\n",
    "#df_json = spark.read.load(path='',format='json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
