{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf78b87-c888-4485-ab9f-f0608b888c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/mahesh/.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/mahesh/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4b6bd9-8181-412d-ad9f-992eb4010c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Object Created\n",
      "No of Partitions: 200\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').appName('Test').getOrCreate()\n",
    "print('Spark Object Created')\n",
    "#print('The HDFS Path is:',spark.conf.get('spark.yarn.appMasterEnv.HDFS_PATH'))\n",
    "print(\"No of Partitions:\",spark.conf.get('spark.sql.shuffle.partitions'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a432a4-cab3-4724-a48c-508459693659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,2013-07-25 00:00:00.0,11599,CLOSED',\n",
       " '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT',\n",
       " '3,2013-07-25 00:00:00.0,12111,COMPLETE',\n",
       " '4,2013-07-25 00:00:00.0,8827,CLOSED',\n",
       " '5,2013-07-25 00:00:00.0,11318,COMPLETE']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the data from the given\n",
    "rdd = spark.sparkContext.textFile('RetailDB SalesData/Orders/part-00000')\n",
    "rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece1f6c8-312b-4123-a065-7849c8d99c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data in the file are: \n",
      "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
      "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
      "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
      "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
      "5,2013-07-25 00:00:00.0,11318,COMPLETE\n",
      "6,2013-07-25 00:00:00.0,7130,COMPLETE\n",
      "7,2013-07-25 00:00:00.0,4530,COMPLETE\n",
      "8,2013-07-25 00:00:00.0,2911,PROCESSING\n",
      "9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\n",
      "10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "# iterate into the above created rdd\n",
    "\n",
    "print('The Data in the file are: ')\n",
    "for c in rdd.take(10):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47029c4-3b6b-4d49-b8ff-0511ccebeed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of partitions in this data:  1\n"
     ]
    }
   ],
   "source": [
    "# to get number of partitions\n",
    "print('The Number of partitions in this data: ',rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc0f0a75-beba-4d34-9a2f-e379325cfa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records:  [68883]\n"
     ]
    }
   ],
   "source": [
    "# to get number of records in the given \n",
    "\n",
    "print('Total Number of records: ',rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b99ba682-6209-4704-bbcc-25027bc0392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Partitions in RDD2:  5\n"
     ]
    }
   ],
   "source": [
    "# to excplicitly doing partiton in the given data\n",
    "\n",
    "rdd2 = spark.sparkContext.textFile('RetailDB SalesData/Orders/part-00000',5)\n",
    "print('The Number of Partitions in RDD2: ',rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d47371de-fea7-4b6a-b3fe-8921996052de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records:  [13984, 13716, 13732, 13730, 13721]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to get number of records in the given each partition\n",
    "\n",
    "print('Total Number of records: ',rdd2.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8184e1-dff8-4ba0-9210-7a50406f7b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Elemenets in List RDD3: \n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# creating rdd using python list\n",
    "\n",
    "lst = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "rdd3 = spark.sparkContext.parallelize(lst)\n",
    "\n",
    "print('The Elemenets in List RDD3: ')\n",
    "for i in rdd3.take(9):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8071e5-2a43-4f75-b1ca-585a2ad5231a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect function is used to print all\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72ecc203-b3be-4451-b307-c06596322f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to automatically input the elemnent\n",
    "lst2 = range(10)\n",
    "lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e25b7c51-6ed1-406d-9302-3daae340b109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to transer one rdd to another\n",
    "new_rdd = rdd3\n",
    "\n",
    "new_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f930bf3-89cc-463b-a518-98b0321e21dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing list comprehension in rdd\n",
    "\n",
    "new_rdd2 = rdd3.map(lambda x:x*2)\n",
    "new_rdd2.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9a307-f5e6-4869-ae6c-a1f2374e3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd in existing dataframe\n",
    "\n",
    "df = spark.createDataFrame(data=[('mahesh',10),('pradeep',11)],schema=['name','age'])\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
